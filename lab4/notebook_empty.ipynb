{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metody probabilistyczne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ryzyko kredytowe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Każda instytucja udzielająca kredytu musi szacować zdolność kredytową i prawdopodobieństwo, czy dana osoba spłaci kredyt. W praktyce większość osób płaci prawidłowo, więc zamiast tego przewiduje się prawdopodobieństwo, że ktoś będzie miał opóźnienia w płatności w określonym czasie, tzw. **probability of default**. Jak sama nazwa wskazuje, nie jest to klasyfikacja binarna, tylko faktyczne prawdopodobieństwo, wykorzystywane przy ocenie ryzyka.\n",
    "\n",
    "Wykorzystamy zbiór z konkursu [Give Me Some Credit](https://www.kaggle.com/c/GiveMeSomeCredit/overview). Jego zmienne to ([źródło](https://www.listendata.com/2019/08/datasets-for-credit-risk-modeling.html)):\n",
    "- `SeriousDlqin2yrs` - brak spłaty raty kredytu przez 90 dni lub dłużej w ciągu ostatnich 2 lat; są to nasze klasy `y`\n",
    "- `RevolvingUtilizationOfUnsecuredLines` - aktualne zapożyczenie na kartach kredytowych i pożyczkach w stosunku do maksymalnego możliwego zapożyczenia\n",
    "- `age` - wiek w latach\n",
    "- `DebtRatio` - aktualne całkowite zapożyczenie względem miesięcznego dochodu\n",
    "- `MonthlyIncome` - miesięczne dochody\n",
    "- `NumberOfOpenCreditLinesAndLoans` - liczba aktualnych pożyczek i kart kredytowych\n",
    "- `NumberOfTimes90DaysLate` - ile razy w przeszłości wystąpiło opóźnienie 90 dni lub dłużej w spłacie raty kredytu\n",
    "- `NumberRealEstateLoansOrLines` - liczba pożyczek z nieruchomością jako zabezpieczeniem\n",
    "- `NumberOfDependents` - liczba pozostałych osób w gospodarstwie domowym\n",
    "- `NumberOfTime30-59DaysPastDueNotWorse` - ile razy w ciągu ostatnich 2 lat nastąpiło opóźnienie w spłacie raty między 30 a 59 dni\n",
    "- `NumberOfTime60-89DaysPastDueNotWorse` - jak wyżej, ale 60-89 dni\n",
    "\n",
    "Ze względu na potrzebę precyzyjnych prawdopodobieństw oraz rozmiar danych (banki mają ogromną liczbę klientów), regresja logistyczna to idealny wybór dla tego zadania. Metryką w takich przypadkach jest AUROC (Area Under Receiver Operating Characteristic curve), bo bierze ona pod uwagę prawdopodobieństwo klasy pozytywnej.\n",
    "\n",
    "Na początek wczytamy i przeanalizujemy wstępnie nasze dane. Jako zbiór testowy wykorzystamy 20% danych. Jako że mamy do czynienia z klasyfikacją niezbalansowaną, to wykorzystamy **podział ze stratyfikacją (stratified split)**, w którym proporcje klasy pozytywnej i negatywnej zostają zachowane w zbiorze treningowym i testowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"give_me_some_credit_data.parquet\")\n",
    "y = df.pop(\"SeriousDlqin2yrs\")\n",
    "X = df\n",
    "\n",
    "y.value_counts(normalize=True).plot.bar(\n",
    "    title=\"Class distribution\", \n",
    "    rot=0, \n",
    "    xlabel=\"Class (defaulted or not)\", \n",
    "    ylabel=\"Class percentage\"\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jako że naszych zmiennych nie jest zbyt wiele, możemy łatwo zweryfikować statystyki naszych danych z pomocą metod `.info()` oraz `.describe()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mamy wartości brakujące w `MonthlyIncome` oraz `NumberOfDependents`. Tutaj z ekonomii wiadomo, że imputacja medianą będzie bardzo dobrym wyborem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W takiej tabeli warto patrzeć na średnią, odchylenie standardowe i maksimum dla każdej cechy. Mamy tutaj ewidentne outliery! Przykładowo, dla `DebtRatio`:\n",
    "- średnio 353\n",
    "- odchylenie standardowe 2037\n",
    "- maksimum >300 tysięcy\n",
    "\n",
    "Trzeba to będzie naprawić. Dodatkowo, nawet abstrahując od tych outlierów, patrząc na statystyki można podejrzewać dość mocno skrzywione rozkłady (co jest dość normalne w ekonomii). W takich sytuacjach lepiej używać tzw. robust statistics, na przykład mediany zamiast średniej, czy też IQR zamiast odchyleń standardowych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 1 (1.5 punktu)**\n",
    "\n",
    "1. Dokonaj imputacji danych medianą (`SimpleImputer`) oraz standaryzacji (`StandardScaler`).\n",
    "2. Wytrenuj regresję logistyczną z domyślnymi hiperparametrami (`LogisticRegression`). Pamiętaj o `n_jobs=-1` (mamy dużo danych) i ustawieniu `random_state=0`. Zmierz AUROC na zbiorze testowym.\n",
    "3. Dodaj usuwanie outlierów pomiędzy imputacją a standaryzacją. Skorzystaj z biblioteki feature-engine oraz klasy `OutlierTrimmer` (domyślne parametry). Wytrenuj na nowo model i porówaj wyniki.\n",
    "4. Dodaj `Winsorizer` pomiędzy usuwaniem outlierów a standaryzacją, który przytnie wartości zmiennych do zakresu między średnią a 3 odchyleniami standardowymi z góry i dołu (po usunięciu outlierów powinny mieć sensowne wartości). Ponownie wytrenuj i przetestuj model.\n",
    "\n",
    "W punkcie 3 filtrujemy także wiersze ze zbioru testowego. Niektórzy uznaliby to za niedopuszczalne, ale warto zwrócić uwagę, że usuwamy tutaj outliery uznawane za błędy, więc z drugiej strony testowanie na nich też daje błędny obraz sytuacji. Można więc to uznać za dopuszczalne, jeżeli dobrze wiemy, co robimy, i estymujemy parametry do usuwania tylko na zbiorze treningowym.\n",
    "\n",
    "Przyda się używać innych nazw zmiennych niż `X_train` i `X_test`, żeby nie nadpisać sobie danych w kolejnych krokach.\n",
    "\n",
    "**Uwaga:**\n",
    "1. Upewnij się, że prawidłowo liczysz AUROC, przekazując do niego prawdopodobieństwa klasy pozytywnej!\n",
    "2. Feature-engine musi być w wersji co najmniej 1.7.0. W razie potrzeby uruchom `pip install -U feature-engine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz mamy faktycznie czysty zbiór danych i wytrenowane pierwsze modele. Warto zwrócić jednak uwagę na dwa fakty:\n",
    "1. Nie dokonaliśmy żadnego tuningu siły regularyzacji, na którą regresja logistyczna jest czuła\n",
    "2. Mamy mocno niezbalansowane klasy, a uwzględniamy je z taką samą wagą"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 2 (1 punkt)**\n",
    "\n",
    "1. Dokonaj tuningu siły regularyzacji:\n",
    "   - sprawdź 100 wartości `C` (odwrotność siły regularyzacji, jak w SVM)\n",
    "   - wykorzystaj `LogisticRegressionCV`\n",
    "   - wybierz model o najlepszej wartości AUROC (`scoring`)\n",
    "   - pamiętaj o `random_state=0`\n",
    "   - sprawdź wynik modelu na zbiorze testowym\n",
    "2. Dodaj zbalansowane wagi klas (`class_weight`), z tuningiem siły regularyzacji jak wyżej. Przetestuj model.\n",
    "3. Dla porównania, wytrenuj las losowy (`RandomForestClassifier`) z 500 drzewami i zbalansowanymi wagami klas, i sprawdź wyniki.\n",
    "4. Skomentuj wyniki:\n",
    "   - czy twoim zdaniem to ogólnie dobry wynik dla takiego problemu?\n",
    "   - czy warto było poświęcić czas na preprocessing danych?\n",
    "   - jak wypadła regresja logistyczna w porównaniu do lasu losowego? Czy jest do dla ciebie oczekiwany, czy zaskakujący wynik?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdźmy jeszcze, jak dobrze nam idzie z predykcjami prawdopodobieństwa. Regresja logistyczna jest z natury dobrze skalibrowana, ale nie ma takiej gwarancji na każdym zbiorze danych. W szczególności może mieć problemy na zbiorach niezbalansowanych, przez bardzo niewielką ilość klasy pozytywnej (a priori jest rzadka).\n",
    "\n",
    "Do rysowania krzywych kalibracji służy klasa `CalibrationDisplay`. W perfekcyjnym przypadku prawdopodobieństwa modelu rosną liniowo raz z \"pewnością\", że punkt należy do klasy pozytywnej. Dla zainteresowanych, [dokumentacja Scikit-learn](https://scikit-learn.org/stable/modules/calibration.html#calibration-curves) omawia te krzywe nieco dokładniej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibrationDisplay\n",
    "\n",
    "clf = ...  # put model variable from last exercise here\n",
    "X_test = ...  # put final X features matrix here\n",
    "y_test = ...  # put final y labels array here\n",
    "\n",
    "CalibrationDisplay.from_estimator(clf, X_test, y_test, n_bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Krótko mówiąc, nie jest dobrze! Nasz model praktycznie zawsze bardzo mocno zaniża prawdopodobieństwo. W połączeniu z wysokim AUROC oznacza to, że w klasyfikacji generalnie nieźle sobie radzi, dla rosnącego prawdopodobieństwa dość szybko przypisuje punkt do klasy pozytywnej, ale finalne wartości prawdopodobieństw są notorycznie zbyt niskie.\n",
    "\n",
    "Do poprawienia tej sytuacji służy `CalibratedClassifierCV`, który na podstawie predykcji naszego modelu oraz zbioru treningowego dotrenowuje dodatkowy model do doprecyzowania prawdopodobieństw. Może być to albo regresja logistyczna, albo tzw. regresja izotoniczna ([isotonic regression](https://scikit-learn.org/stable/modules/isotonic.html#isotonic)) - specjalny model wykorzystujący fakt, że prawdopodobieństwa są stricte rosnące od 0 do 1.\n",
    "\n",
    "**Zadanie 3 (1 punkt)**\n",
    "\n",
    "Wytrenuj kalibrator prawdopodobieństwa z pomocą `CalibratedClassifierCV`. Wykorzystaj tryb regresji izotonicznej i 10-krotną walidację skrośną. Przekaż do niego wytrenowany estymator wykorzystany w poprzednim ćwiczeniu. Narysuj krzywą kalibracji na zbiorze testowym, jak w poprzednim przykładzie.\n",
    "\n",
    "Skomentuj, jak udało się poprawić kalibrację modelu. Czy twoim zdaniem taki model przewiduje już dość dobrze prawdopodobieństwa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detekcja spamu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykorzystamy teraz klasyfikator Naiwnego Bayesa do klasyfikacji binarnej tekstów - czy email jest spamem, czy też nie (tzw. \"ham\"). Takie zadanie nazywa się też spam/ham classification. Zbiorem będzie [Enron Spam dataset](http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/index.html), zebrany na potrzeby artykułu:\n",
    "\n",
    "> Metsis, Vangelis, Ion Androutsopoulos, and Georgios Paliouras. *\"Spam filtering with Naive Bayes - which Naive Bayes?.\"* CEAS. Vol. 17. 2006. [link](https://userweb.cs.txstate.edu/~v_m137/docs/papers/ceas2006_paper_corrected.pdf)\n",
    "\n",
    "[Upadek firmy energetycznej Enron](https://userweb.cs.txstate.edu/~v_m137/docs/papers/ceas2006_paper_corrected.pdf) i skandal wywołany wiadomościami o nieprawidłowościach finansowych spowodowały, że podczas śledztwa i procesu zmuszono firmę do upublicznienia pełnej zawartości skrzynek mailowych 150 pracowników. Jest to doskonałe źródło prawdziwych danych, typowych emaili niebędących spamem, i było wykorzystywane wielokrotnie na potrzeby tworzenia wielu różnych zbiorów danych. Na potrzeby powyższego artykułu wybrano 6 pracowników z dużą liczbą emaili.\n",
    "\n",
    "Maile będące spamem otrzymano z 4 różnych źródeł:\n",
    "1. [Zbioru danych SpamAssassin](https://spamassassin.apache.org/old/publiccorpus/) - stworzony z pomocą [Apache SpamAssassin](https://spamassassin.apache.org/), narzędzia open source do detekcji spamu. Maile niebędące spamem są w nim bardzo specyficzne (pochodzą ze skrzynek fundacji Apache i dotyczą praktycznie tylko programownaia), za to spam jest uniwersalny.\n",
    "2. [Project Honeypot](https://www.projecthoneypot.org/) - projekt mający na celu wykrywanie spammerów (w tym przez specjalne pułapki, tzw. spam baiting) i tworzenie baz do filtrowania spamu.\n",
    "3. [Bazę spamu Bruce'a Guentera](http://untroubled.org/spam/) - gromadzi on cały spam, który dostaje na skrzynki (w tym takie do spam baitingu) od 1997 roku.\n",
    "4. Maile ze spamem zgromadzone przez trzeciego autora artykułu, Georgiosa Paliourasa.\n",
    "\n",
    "Artykuł prezentuje spersonalizowane kolekcje maili, dla każdego z 6 wybranych pracowników Enronu. My natomiast połączymy wszystkie dane i stworzymy prosty, uniwersalny filtr. Nasz filtr będzie operował tylko na tytułach i treści maili - realnie wdrażane systemy są dużo bardziej rozbudowane i wykorzystują metadane, bazy adresów, analizę tagów HTML i XML etc.\n",
    "\n",
    "Wykorzystamy przetworzoną postać tego zbioru w postaci pliku CSV z interesującymi nas elementami. Skrypt do tego wzięto [z tego repozytorium na Githubie](https://github.com/MWiechmann/enron_spam_data?fbclid=IwAR1icPxic8TDD6HGk6empohwcZ76uig0NxwxK7sWbczinBNfxEEt-ETparA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"enron_spam_data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na pierwszy rzut oka widać parę problemów:\n",
    "- kolumny `Message ID` i `Date` są niepotrzebne\n",
    "- niektóre wiadomości mają tylko tytuł lub tylko wiadomość\n",
    "- klasa jest zapisana jako string spam/ham\n",
    "\n",
    "**Zadanie 4 (0.5 punktu)**\n",
    "\n",
    "1. Uzupełnij wartości brakujące w kolumnach `Subject` i `Message` pustymi stringami.\n",
    "2. Połącz kolumny `Subject` i `Message` w jedną kolumnę `text`, ze spacją pomiędzy nimi. Może się przydać metoda `pd.Series.str.cat()`. Wyodrębnij ją do nowej zmiennej `texts`.\n",
    "3. Zamień wartość `ham` na 0 i `spam` na 1 w kolumnie `Spam/Ham`. Wyodrębnij ją do nowej zmiennej `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdźmy jeszcze rozkład klas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_percentages = y.value_counts(normalize=True)\n",
    "class_percentages = round(class_percentages * 100, 2)\n",
    "\n",
    "ax = class_percentages.plot.bar(title=\"Class distribution\", rot=0)\n",
    "\n",
    "ax.bar_label(ax.containers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten zbiór w całości jest o tyle specyficzny, że dodano tutaj aż tyle spamu, że jest go nawet więcej niż zwykłych emaili. W praktyce spamu jest dużo mniej, ok. 1/3 maili. Tutaj uznajemy, że klasy są zbalansowane.\n",
    "\n",
    "Typową metryką w klasyfikacji spamu jest **precyzja (precision)**, bo wyznacza ona, jak dobrzy jesteśmy w oznaczaniu spamu. Recall ma mniejsze znaczenie, bo bardziej zależy nam na minimalizacji FP - ważne maile nie mogą trafiać do spamu, natomiast jeżeli spam czasem trafi do zwykłej skrzynki (FN), to nie ma wielkiego znaczenia. Często używa się też F1-score jako metryki.\n",
    "\n",
    "W przypadku tych metryk (precision/recall/F1-score) zawsze trzeba sprawdzić co najmniej dwie z nich, żeby uniknąć przypadkiem sytuacji, że mamy np. precision bliskie 100%, a dramatycznie niski recall - to też nie byłoby akceptowalne. Trzeba też uważać podczas walidacji skrośnej - np. klasyfikator przewidujący cały czas klasę pozytywną miałby 100% recall, a cały czas negatywną 100% precision, więc typowo najwygodniej jest optymalizować F1-score.\n",
    "\n",
    "Na początek podzielimy zbiór na treningowy i testowy. Naszymi cechami będą zliczenia wystąpień poszczególnych słów, obliczone z pomocą klasy `CountVectorizer` ([dokumentacja](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)). Na start podamy `binary=True`, dzięki czemu cechy będą binarne - czy słowo wystąpiło, czy nie. Dzięki temu będziemy mogli użyć Naiwnego Bayesa z rozkładem dwumianowym - `BernoulliNB`. Wykonamy minimalny preprocessing, usuwając tylko tzw. stop words, czyli \"nudne\", niepotrzebne słowa, typu \"he\", \"she\", \"the\" etc.\n",
    "\n",
    "Zauważ, że wejściem do metody `.fit()` jest lista tekstów, ze względu a domyślne `input=\"content\"`. Można by tu też wskazać listę ścieżek do plików z tekstami (`input=\"filename\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import f1_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "\n",
    "texts_train, texts_test, y_train, y_test = train_test_split(\n",
    "    texts,\n",
    "    y,\n",
    "    test_size=0.25,\n",
    "    random_state=0,\n",
    "    stratify=y,\n",
    ")\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    binary=True,\n",
    ")\n",
    "count_vectorizer.fit(texts_train)\n",
    "\n",
    "X_train = count_vectorizer.transform(texts_train)\n",
    "X_test = count_vectorizer.transform(texts_test)\n",
    "\n",
    "print(f\"Dictionary size: {X_train.shape[1]}\")\n",
    "\n",
    "clf_nb_bernoulli = BernoulliNB(binarize=None)\n",
    "clf_nb_bernoulli.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf_nb_bernoulli.predict(X_test)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precision: {100 * precision:.2f}%\")\n",
    "print(f\"F1-score: {100 * f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wynik jest prawie doskonały. Może uda się jeszcze wycisnąć ten ostatni procent?\n",
    "\n",
    "**Zadanie 5 (2 punkty)**\n",
    "\n",
    "1. Oblicz wystąpienia słów bez binaryzacji, otrzymując drugi zbiór cech.\n",
    "2. Wytrenuj model multinomial Naive Bayes na otrzymanym zbiorze. Sprawdź precyzję i F1-score na zbiorze testowym.\n",
    "3. Stwórz `Pipeline` z połączenia `CountVectorizer` i `MultinomialNB`. Podaj te opcje, co wcześniej do CountVectorizera.\n",
    "4. Dokonaj tuningu hiperparametrów:\n",
    "   - `ngram_range` - jakie n-gramy rozważać, sprawdź wartości `[(1, 1), (1, 2), (1, 3)]` (same słowa, + bigramy, + bigramy i trigramy)\n",
    "   - `max_df` - w jakim maksymalnie ułamku tekstów w zbiorze może wystąpić słowo, sprawdź wartości `[0.5, 0.6, 0.7, 0.8, 0.9, 1.0]`\n",
    "   - wybierz model o najwyższej precyzji\n",
    "   - użyj standardowych ustawień: 5-krotna walidacja skrośna, `n_jobs=-1`\n",
    "   - sprawdź precyzję i F1-score na zbiorze testowym dla znalezionych optymalnych hiperparametrów\n",
    "5. Wypisz optymalne hiperparametry. Sprawdź, jak zmienił się rozmiar słownika przy tych hiperparametrach.\n",
    "6. Skomentuj uzyskane hiperparametry. Jaki zakres (kontekst) uwzględniamy przy takich n-gramach? Czy uwzględniamy wszystkie słowa/bigramy/trigramy?\n",
    "\n",
    "**Uwaga:**\n",
    "1. Do metody `.fit()` możesz przekazać `texts_train` i `y_train`. Jako że `CountVectorizer` ma interfejs transformera (ma metodę `.transform()`), to Scikit-learn przekształci nim teksty do macierzy `X_train` i to ją przekaże dalej do klasyfikatora.\n",
    "2. Przy tworzeniu pipeline'u może się przydać [ta odpowiedź na StackOverflow](https://stackoverflow.com/a/64258746/9472066).\n",
    "3. Pamiętaj, że do testowania też trzeba przekazać teksty, które przekształcimy w środku pipeline'u."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W przypadku tak wysokich wyników raczej nie ma sensu sprawdzać bardziej złożonych technik, typu ważenie słów (TF-IDF, BM25), upraszczanie słów (stemming, lematyzacja) etc. W przypadku bardziej złożonych tekstów mogłoby to być przydatne.\n",
    "\n",
    "Naiwnego Bayesa do klasyfikacji tekstu używa się obecnie relatywnie rzadko. Potrafi on jednak działać równie dobrze, jak transformery, kiedy dla danego zadania mają znaczenie same wystąpienia słów czy tematyka tekstu, a nie dokładna semantyka zdań. Naiwny Bayes jest przy tym oczywiście bez porównania prostszy i szybszy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dane z ekspresji genów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naszym ostatnim zbiorem będzie [GSE2034](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=gse2034) ([Mendeley Data](https://data.mendeley.com/datasets/v3cc2p38hb/1), [Kaggle](https://www.kaggle.com/datasets/saurabhshahane/gene-expression-profiles-of-breast-cancer)). Jest to zbiór ekspresji genów z sekwencjonowania RNA, zebrany na potrzeby artykułu:\n",
    "\n",
    "> Wang, Yixin, et al. *\"Gene-expression profiles to predict distant metastasis of lymph-node-negative primary breast cancer.\"* The Lancet 365.9460 (2005): 671-679. [link](https://www.ihes.fr/~zinovyev/princmanif2006/Wang_lancet_2005.pdf)\n",
    "\n",
    "Zbiór ten został stworzony z danych pochodzących z sekwencjonowania RNA 286 pacjentek chorych na nowotwór piersi bez przerzutów do węzłów chłonnych ([lymph node-negative breast cancer](https://www.komen.org/breast-cancer/diagnosis/stages-staging/lymph-node-status/)). Celem było sprawdzenie, czy na podstawie analizy genetycznej można przewidzieć, czy nastąpi odległa metastaza nowotworu ([distant metastatis](https://training.seer.cancer.gov/staging/systems/summary/distant.html)), czyli przerzuty nowotworu przez krew do odległych części ciała. Takie przypadki uznaje się za porażkę w leczeniu nowotworu. Według źródeł z artykułu, ok. 85-90% pacjentek jest poddawane dodatkowej terapii (np. chemoterapii) po operacji usunięcia nowotworu, natomiast personalizowana medycyna oparta o analizę genetyczną pozwoliłaby nie tylko oszczędzić tego leczenia (z ciężkimi skutkami ubocznymi) niektórym pacjentkom, ale też skupić się bardziej na innych, z większym ryzykiem przerzutów.\n",
    "\n",
    "Cechy w tym zbiorze to poziom ekspresji 12634 genów w sekwencjonowanym RNA. Ekspresja genu ([gene expression](https://en.wikipedia.org/wiki/Gene_expression)) to proces tłumaczenia informacji genetycznej z genu (sekwencji DNA) do białek (przez kodujące RNA) lub innych funkcji (przez niekodujące RNA). Im silniejsza ekspresja genu, tym więcej materiału (zwykle białek / protein) produkuje dany gen w danym organizmie. Jest to najmniejszy molekularnie poziom, na którym można zaobserwować fenotyp, czyli cechy stworzone na podstawie genotypu.\n",
    "\n",
    "Oprogramowanie do analizy ekspresji genów dokonuje od razu standaryzacji poziomu ekspresji każdego z genów. W związku dla każdego z genów mamy jego Z-score, czyli wartość cechy oznacza, o ile odchyleń standardowych jego wartość różni się od średniej ekspresji tego genu wśród wszystkich próbek. Jest to wygodne do analiz biomedycznych, bo pozwala bezpośrednio porównywać ze sobą pacjentów. Przy okazji jest też wygodne do uczenia maszynowego.\n",
    "\n",
    "Zmienną do przewidywania jest tutaj, czy nastąpi nawrót nowotworu i przerzuty w ciągu 5 lat od operacji (to podczas niej pobiera się tkankę nowotworową do badań genetycznych). Warto zwrócić uwagę, że mamy tutaj 286 pacjentek, a ponad 12 tysięcy cech, a zatem $n >> d$.\n",
    "\n",
    "W przypadku danych medycznych typowo optymalizowaną metryką jest **czułość (recall)**, ponieważ zależy nam na tym, żeby minimalizować przypadki FN. Oznaczają one brak wykrycia choroby, co ma tragiczne skutki, w przeciwieństwie do FP, które może mieć co prawda negatywne skutki (np. dodatkowe badania, niepotrzebne leczenie ze skutkami ubocznymi), ale typowo mniejsze. Oczywiście dalej trzeba sprawdzić F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 6 (0.5 punktu)**\n",
    "\n",
    "1. Wczytaj dane z plików `GSE2034-Normal.tsv` oraz `GSE2034-Tumor.tsv`. Są one w formacie TSV, czyli CSV, ale z tabami (`\\t`) zamiast przecinków.\n",
    "2. Usuń kolumnę `Hybridization REF` z obu ramek.\n",
    "3. Obecnie kolumny to pacjentki, a wiersze to ekspresje genów. Dokonaj transpozycji (transpose) DataFrame'ów tak, aby kolumnami były cechy.\n",
    "4. Połącz oba DataFrame'y w jedną tablicę Numpy'a `X`, reprezentującą cały zbiór danych.\n",
    "5. Stwórz wektor klas `y`, mający wartości 0 dla wierszy z pliku `Normal` i wartość 1 dla wierszy z pliku `Tumor`.\n",
    "6. Narysuj wykres częstości klas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać, mamy więcej przykładów pozytywnych niż negatywnych. Jest to dość częste w zbiorach medycznych, które są często tworzone bardziej do analizy przypadków chorobowych, niż do tworzenia modeli predykcyjnych. Oczywiście to, że klasy pozytywnej jest więcej, wcale nie oznacza, że klasyfikacja musi być prostsza.\n",
    "\n",
    "Jako że nasze cechy są ciągłe, to musimy użyć klasyfikatora Naiwnego Bayesa z założeniem rozkładu normalnego do klasyfikacji. Dokonamy podziału na zbiór treningowy i testowy w proporcjach 67%-33%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.33,\n",
    "    random_state=0,\n",
    "    stratify=y,\n",
    ")\n",
    "\n",
    "clf_nb_gaussian = GaussianNB()\n",
    "clf_nb_gaussian.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf_nb_gaussian.predict(X_test)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Recall: {100 * recall:.2f}%\")\n",
    "print(f\"F1-score: {100 * f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyniki są co najwyżej mocno przeciętne. Można powiedzieć, że mamy tu underfitting, ale przy tej ilości danych (zbiór treningowy ma ok. 200 próbek) użycie dużo pojemniejszego modelu mogłoby się łatwo skończyć się mocnym overfittingiem.\n",
    "\n",
    "Może to nasze założenia co do normalności klas są błędne? Sprawdźmy to dla kilku pierwszych cech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X[:, :9]).hist()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O ile niektóre rozkłady nieco przypominają normalny, co tłumaczy nie najgorszy wynik klasyfikatora, to jednak daleko do pełnej normalności. Empiryczne oszacowanie rozkładu za pomocą Kernel Density Estimation (KDE) powinno poprawić wynik w takiej sytuacji.\n",
    "\n",
    "Scikit-learn nie ma gotowego Naive Bayesa z KDE, ale łatwo go zaimplementować. W tym celu trzeba stworzyć własny klasyfikator. Robi się w skrócie to tak ([dokładny tutorial](https://scikit-learn.org/stable/developers/develop.html)):\n",
    "1. Tworzymy klasę dziedziczącą po `BaseEstimator`, `ClassifierMixin` oraz ewentualnie innych klasach, które nas interesują.\n",
    "2. Wszystkie hiperparametry są argumentami konstruktora. Każdy jest przypisywany, bez żadnych zmian, do atrybutu o dokładnie tej samej nazwie.\n",
    "3. Implementujemy metody `.fit(X, y)`,`.predict(X)`, oraz ewentualnie `.predict_proba(X)`. Metoda `.fit()` musi zwracać `self`.\n",
    "4. Na początku metody `.fit(X, y)` używamy funkcji `check_X_y()`, żeby dokonać podstawowej walidacji wejścia. Na początku metody `.predict(X)` analogicznie używamy funkcji `check_array()` na wejściu `X`.\n",
    "\n",
    "Taki interfejs gwarantuje, że nasz własny model będzie mógł być używany np. w `GridSearchCV`.\n",
    "\n",
    "**Zadanie 7 (2.5 punktu)**\n",
    "\n",
    "Uzupełnij implementację klasy `KernelDensityEstimationNB`, realizującej algorytm KDE Naive Bayes.\n",
    "\n",
    "1. Zakładamy, że klasy są liczbami całkowitymi, z wartościami `[0, 1, ..., liczba klas - 1]`.\n",
    "2. Rozkład a priori (prior) dla każdej klasy to proporcja tej klasy w zbiorze treningowym. Naive Bayes używa log-sum-exp trick, więc można od razu obliczyć logarytm i zapisać log-prior.\n",
    "3. Każda klasa ma swój model Kernel Density Estimation, wytrenowany na przykładach z tej klasy.\n",
    "4. Podczas predykcji trzeba obliczyć, dla każdej z nowych próbek do zaklasyfikowania, jej wiarygodność (likelihood) dla każdej z klas. Służy do tego metoda `.score_samples()`. Ze względu na log-sum-exp trick ponownie oblicza się logarytm, otrzymując log-likelihood.\n",
    "5. Dla każdego z przykładów, dla każdej z klas, sumujemy log-prior i log-likelihood, otrzymując wartość rozkładu a posteriori (posterior). Zgodnie z zasadą maximum a posteriori (MAP), wybieramy klasę o najwyższej z tych wartości. Może tu się przydać `np.argmax()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.utils import check_array, check_X_y\n",
    "\n",
    "\n",
    "class KernelDensityEstimationNB(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Naive Bayes with KDE estimation of feature distributions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    kernel : str\n",
    "        Kernel name, passed to KernelDensity\n",
    "\n",
    "    bandwidth : float\n",
    "        Kernel bandwidth\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel: str = \"gaussian\", bandwidth: float = 1.0):\n",
    "        # create attributes\n",
    "        self.kernel = kernel\n",
    "        self.bandwidth = bandwidth\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # make sure that we are working on Numpy arrays, and perform basic\n",
    "        # sanity checks\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        X, y = check_X_y(X, y)\n",
    "\n",
    "        # make sure that y has values from 0 to num_classes - 1\n",
    "        assert y.min() == 0\n",
    "        assert y.max() == len(np.unique(y)) - 1\n",
    "\n",
    "        # vector of possible classes\n",
    "        self.classes_ = np.sort(np.unique(y))\n",
    "\n",
    "        # vector for remembering class priors, i.e. proportion of classes\n",
    "        # in the training set\n",
    "        self.log_priors_ = np.empty(len(self.classes_))\n",
    "\n",
    "        # list of KernelDensity models, one for each class\n",
    "        self.kde_models_ = []\n",
    "\n",
    "        for class_idx in self.classes_:\n",
    "            # select X and y from the current class\n",
    "            class_mask = ...\n",
    "            X_class = ...\n",
    "            y_class = ...\n",
    "\n",
    "            # calculate log-prior, i.e. logarithm of current class proportion\n",
    "            self.log_priors_[class_idx] = ...\n",
    "\n",
    "            # create and fit KernelDensity estimator, using given kernel and bandwidth\n",
    "            kde = ...\n",
    "\n",
    "            # save KDE\n",
    "            ...\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # make sure that we are working on Numpy array, and perform basic\n",
    "        # sanity checks\n",
    "        X = np.array(X)\n",
    "        X = check_array(X)\n",
    "\n",
    "        # use KDE models and calculate log-likelihoods for each sample\n",
    "        log_likelihoods = ...\n",
    "\n",
    "        # calculate posterior probability: log-prior + log-likelihood\n",
    "        # make sure that they have the same shape (transpose if needed)\n",
    "        posteriors = ...\n",
    "\n",
    "        # select the index of class with maximum a posteriori (MAP) rule\n",
    "        map_class = ...\n",
    "\n",
    "        return map_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_nb_kde = KernelDensityEstimationNB()\n",
    "clf_nb_kde.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf_nb_kde.predict(X_test)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Recall: {100 * recall:.2f}%\")\n",
    "print(f\"F1-score: {100 * f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W przypadku czułości (recall) jest już zdecydowanie lepiej! F1-score poprawił się, ale tylko trochę - precyzja mogłaby być lepsza. Ale KDE jest dość czułe na dobór szerokości (bandwidth) kernela, więc tuning hiperparametrów powinien tutaj pomóc.\n",
    "\n",
    "Ciekawą opcją dla szerokości kernela jest **Silverman's rule of thumb**. Jest to prosty wzór na szerokość kernela przy założeniu, że rozkład jest unimodalny. Scikit-learn wspiera tę opcję przez przekazanie wartości `bandwidth=\"silverman\"`. Wzór dla wielu cech ([Wikipedia](https://en.wikipedia.org/wiki/Multivariate_kernel_density_estimation#Rule_of_thumb)):\n",
    "\n",
    "$$\n",
    "h_{silverman} = \\left(\\frac{4}{d+2}\\right) n^{\\frac{-1}{d+4}} \\sigma\n",
    "$$\n",
    "\n",
    "Gdzie $\\sigma$ to wektor odchyleń standardowych cech, $n$ to liczba próbek, a $d$ to liczba wymiarów.\n",
    "\n",
    "Wadą implementacji w Scikit-learn jest to, że nie uwzględnia on odchyleń $\\sigma$, więc bandwidth jest wspólny dla wszystkich cech. Jeżeli jednak są one w podobnej skali (tak jak w tym zbiorze), to nie stanowi to dużego problemu. No i w razie potrzeby można zawsze ustandardyzować dane.\n",
    "\n",
    "Dla zainteresowanych - tuning kernel bandwidth to mocno badane przez matematyków zagadnienie, i istnieje do niego szereg rozbudowanych metod statystycznych. [Ten tutorial](https://aakinshin.net/posts/kde-bw/) jest chyba najlepszym i najbardziej czytelnym, a przy tym ma potężny przegląd literatury."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_nb_kde = KernelDensityEstimationNB(bandwidth=\"silverman\")\n",
    "clf_nb_kde.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf_nb_kde.predict(X_test)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Recall: {100 * recall:.2f}%\")\n",
    "print(f\"F1-score: {100 * f1:.2f}%\")\n",
    "\n",
    "class_0_bandwidth = clf_nb_kde.kde_models_[0].bandwidth_\n",
    "class_1_bandwidth = clf_nb_kde.kde_models_[1].bandwidth_\n",
    "\n",
    "print(f\"Class bandwidths: {class_0_bandwidth:.4f}, {class_1_bandwidth:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wynik się poprawił, a ilość pracy była minimalna. Jeżeli nasze rozkłady cech wyglądają na unimodalne i podobne do normalnego, to zasada Silvermana może w prosty i szybki sposób ulepszyć nasze wyniki.\n",
    "\n",
    "Dodatkowo widać, że wyestymowany bandwidth jest bardzo bliski 1 (korekta jest zasadniczo kosmetyczna), więc w tym obszarze można ewentualnie szukać lepszych wartości hiperparametru.\n",
    "\n",
    "**Zadanie 9 (1 punkt)**\n",
    "\n",
    "1. Dokonaj tuningu hiperparametru `bandwidth`:\n",
    "   - sprawdź 100 wartości na siatce liniowej w zakresie od 0.75 do 1.25\n",
    "   - użyj kernela gaussowskiego\n",
    "   - wybierz model o najwyższym F1-score\n",
    "   - użyj standardowych ustawień: 5-krotna walidacja skrośna, `random_state=0`, `n_jobs=-1`\n",
    "2. Wypisz znalezioną optymalną wartość szerokości kernela.\n",
    "3. Sprawdź czułość (recall) i F1-score na zbiorze testowym dla znalezionych optymalnych hiperparametrów.\n",
    "4. Czy optymalna wartość kernela była większa, czy mniejsza od domyślnego 1? Czy takie oszacowanie KDE ma bardziej gładki kształt (większy bias), czy bardziej zróżnicowany (większa wariancja)?\n",
    "4. Czy udało się poprawić wynik? Czy takie wartości metryk uważasz za zadowalające?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie dodatkowe (3 punkty)\n",
    "\n",
    "Najpopularniejszą techniką nieparametryczną, która pozwala oszacować szerokość kernela, jest **estymator Sheathera-Jonesa (Sheather-Jones estimator)**. Używa on dość zaawansowanej statystyki, aby zminimalizować błąd średniokwadratowy między oszacowaniem KDE a prawdziwym (nieznanym) rozkładem. Typowo daje dużo lepsze wyniki od reguły Silvermana, bo wiele cech nie jest unimodalnych i/lub zbliżonych do rozkładu normalnego.\n",
    "\n",
    "Dodatkowo KDE w postaci ze Scikit-learn, używające wprost definicji i opierające się na najbliższych sąsiadach, nie jest zbyt szybkie. Można pokazać, że sumowanie kerneli z każdego punktu odpowiada operacji **konwolucji (convolution)**, a dzięki temu można wykorzystać **twierdzenie o splocie (convolution theorem)** i obliczyć to za pomocą szybkiej transformaty Fouriera (FFT). Algorytm FFTKDE jest rzędy wielkości szybszy od tradycyjnego podejścia, i świetnie się skaluje.\n",
    "\n",
    "Oba te mechanizmy implementuje biblioteka [KDEpy](https://kdepy.readthedocs.io/en/latest/index.html), z interfejsem bardzo podobnym do Scikit-learn.\n",
    "\n",
    "1. Zaimplementuj KDE Naive Bayesa z pomocą algorytmu FFTKDE.\n",
    "2. Zastosuj estymator Sheathera-Jonesa do oszacowania szerokości kernela. Czy wynik jest lepszy, czy gorszy od tego z zasady Silvermana oraz z walidacji skrośnej?\n",
    "3. Wczytaj zbiór Cod-RNA z laboratorium 3. Wytrenuj na nim algorytmy:\n",
    "   - Gaussian NB\n",
    "   - KDE NB\n",
    "   - FFTKDE NB\n",
    "4. Porównaj wyniki oraz czas treningu. Czy warto było tutaj zastosować algorytm FFTKDE?\n",
    "5. Dokonaj tuningu szerokości kernela za pomocą metod: Silvermana, Sheathera-Jonesa oraz walidacji skrośnej. Które daje najlepszy wynik?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
